{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b06b2efd-9c2d-498f-b2f4-9ea189f4c0d8",
   "metadata": {},
   "source": [
    "#### Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8f7b692-e4a9-466d-8cce-f0fe41f7dce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import syllables\n",
    "import re\n",
    "import nltk\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61534cfe-e696-413f-b4af-95e0ee32f148",
   "metadata": {},
   "source": [
    "#### Upload input file using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "391e31ce-9af8-4766-9eb8-446498c5e3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Input (1).xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530667ae-74b2-46f6-bfa5-8ebb343224a9",
   "metadata": {},
   "source": [
    "#### Setp 1 :- Extracting Article Content from list of URLs using Selenium webdriver and BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3db8eb08-fd58-4856-9505-651496ff52a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Chrome WebDriver instance\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "\n",
    "# Initialize an empty list to store the data\n",
    "data = []\n",
    "\n",
    "# Loop through each URL in the DataFrame column \"URL\"\n",
    "for url in df[\"URL\"]:\n",
    "    \n",
    "    driver.get(url)\n",
    "    \n",
    "    # Pause execution for 2 seconds to allow page loading\n",
    "    \n",
    "    time.sleep(2)\n",
    "    page = driver.page_source\n",
    "    soup = BS(page, \"html.parser\")\n",
    "    \n",
    "    try :\n",
    "        title = driver.find_element(by = \"xpath\", value = '//h1[contains(@class,\"title\")]').text\n",
    "    except:\n",
    "        title = \"NotAvailable\"\n",
    "    \n",
    "    try:\n",
    "        article = '\\n'.join([i.text for i in soup.find_all(['p', 'h2'], class_=None)])\n",
    "    except:\n",
    "        atticle = \"NotAvailable\"\n",
    "\n",
    "    content = title + \". \" + article\n",
    "\n",
    "    data.append(content)\n",
    "    time.sleep(1)\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Assign the collected data to the \"Article Content\" column in the DataFrame\n",
    "df[\"Article Content\"] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5caae37-34c0-4235-97e8-533c5ec6f0a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>Article Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "      <td>Rise of telemedicine and its Impact on Livelih...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>321.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-e-hea...</td>\n",
       "      <td>Rise of e-health and its impact on humans by t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2345.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-e-hea...</td>\n",
       "      <td>Rise of e-health and its impact on humans by t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4321.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "      <td>Rise of telemedicine and its Impact on Livelih...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>432.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "      <td>Rise of telemedicine and its Impact on Livelih...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   URL_ID                                                URL  \\\n",
       "0   123.0  https://insights.blackcoffer.com/rise-of-telem...   \n",
       "1   321.0  https://insights.blackcoffer.com/rise-of-e-hea...   \n",
       "2  2345.0  https://insights.blackcoffer.com/rise-of-e-hea...   \n",
       "3  4321.0  https://insights.blackcoffer.com/rise-of-telem...   \n",
       "4   432.0  https://insights.blackcoffer.com/rise-of-telem...   \n",
       "\n",
       "                                     Article Content  \n",
       "0  Rise of telemedicine and its Impact on Livelih...  \n",
       "1  Rise of e-health and its impact on humans by t...  \n",
       "2  Rise of e-health and its impact on humans by t...  \n",
       "3  Rise of telemedicine and its Impact on Livelih...  \n",
       "4  Rise of telemedicine and its Impact on Livelih...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb59b36-0ba7-449c-a534-a6fef8379e93",
   "metadata": {},
   "source": [
    "#### Step 2:-\n",
    "##### Once we have extracted Artiicle content and saved data in DataFrame, It's time to move to next step :- Uploading Required files for Data Analysis\n",
    "###### Here I have uploaded required files for Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3705771b-b69d-4920-b47d-adb4302d74d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files which contains Positive and Negative words :- \n",
    "\n",
    "positive_words = pd.read_csv(\"positive-words.txt\", index_col=False)\n",
    "negative_words = pd.read_csv(\"negative-words.txt\", encoding='latin1', header=None)\n",
    "negative_words = negative_words.rename(columns={0: \"ABCD\"})\n",
    "\n",
    "# Files which contains Stop Words from :-  \n",
    "\n",
    "Auditor_SW = pd.read_csv(\"StopWords_Auditor.txt\")['ERNST'].tolist()\n",
    "Currencies_SW = pd.read_csv(\"StopWords_Currencies.txt\", delimiter='|', encoding='latin1')['AFGHANI  '].tolist()\n",
    "DatesandNumbers_SW = pd.read_csv(\"StopWords_DatesandNumbers.txt\", delimiter='|')['HUNDRED  '].tolist()\n",
    "Generic_SW = pd.read_csv(\"StopWords_Generic.txt\")['ABOUT'].tolist()\n",
    "GenericLong_SW = pd.read_csv(\"StopWords_GenericLong.txt\")['a'].tolist()\n",
    "Geographic_SW = pd.read_csv(\"StopWords_Geographic.txt\", delimiter='|')['UNITED  '].tolist()\n",
    "Names_SW = pd.read_csv(\"StopWords_Names.txt\", delimiter='|')['SMITH  '].tolist()\n",
    "\n",
    "# Combine all the stop words into a single set to make the analytical process easy\n",
    "stop_words = set(\n",
    "    Auditor_SW + Currencies_SW + DatesandNumbers_SW + Generic_SW +\n",
    "    GenericLong_SW + Geographic_SW + Names_SW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00aa5a27-6bfc-4a56-98ec-0670b7e3117b",
   "metadata": {},
   "source": [
    "#### Step 3 :- Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "876b666c-ffb8-4333-b340-8c05cdbfff1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_positive_score(text):\n",
    "    positive_words_in_text = [word for word in word_tokenize(text) if word.lower() in positive_words['a+'].values]\n",
    "    return len(positive_words_in_text)\n",
    "\n",
    "# Function to calculate negative score\n",
    "def calculate_negative_score(text):\n",
    "    negative_words_in_text = [word for word in word_tokenize(text) if word.lower() in negative_words['ABCD'].values]\n",
    "    return len(negative_words_in_text)\n",
    "\n",
    "# Function to calculate word count\n",
    "def count_total_words(text):\n",
    "    words = word_tokenize(text)\n",
    "    return len(words)\n",
    "\n",
    "# Function to calculate complex word count\n",
    "def count_complex_words(text):\n",
    "    complex_words = [word for word in word_tokenize(text) if syllables.estimate(word) > 2]\n",
    "    return len(complex_words)\n",
    "\n",
    "# Function to calculate syllables per word\n",
    "def count_syllables_per_word(text):\n",
    "    words = word_tokenize(text)\n",
    "    syllables_per_word = sum(syllables.estimate(word) for word in words)\n",
    "    return syllables_per_word\n",
    "\n",
    "# Function to calculate average sentence length\n",
    "def calculate_avg_sentence_length(text):\n",
    "    sentences = re.split(r'[.!?]', text)\n",
    "    total_sentences = len(sentences)\n",
    "    total_words = count_total_words(text)\n",
    "    return total_words / total_sentences\n",
    "\n",
    "# Function to calculate percentage of complex words\n",
    "def calculate_percentage_complex_words(text):\n",
    "    total_words = count_total_words(text)\n",
    "    complex_words = count_complex_words(text)\n",
    "    return (complex_words / total_words) * 100\n",
    "\n",
    "# Function to calculate Fog Index\n",
    "def calculate_fog_index(avg_sentence_length, percentage_complex_words):\n",
    "    return 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "\n",
    "# Function to calculate average number of words per sentence\n",
    "def calculate_avg_words_per_sentence(text):\n",
    "    total_words = count_total_words(text)\n",
    "    sentences = re.split(r'[.!?]', text)\n",
    "    total_sentences = len(sentences)\n",
    "    return total_words / total_sentences\n",
    "\n",
    "# Function to count personal pronouns\n",
    "def count_personal_pronouns(text):\n",
    "    personal_pronouns = ['I', 'we', 'my', 'ours', 'us']\n",
    "    pronoun_count = sum(1 for word in word_tokenize(text) if word.lower() in personal_pronouns)\n",
    "    return pronoun_count\n",
    "\n",
    "# Function to calculate average word length\n",
    "def calculate_avg_word_length(text):\n",
    "    total_words = count_total_words(text)\n",
    "    total_characters = sum(len(word) for word in word_tokenize(text))\n",
    "    return total_characters / total_words\n",
    "\n",
    "# Apply functions to DataFrame\n",
    "df['Positive_Score'] = df['Article Content'].apply(calculate_positive_score)\n",
    "df['Negative_Score'] = df['Article Content'].apply(calculate_negative_score)\n",
    "df['Word Count'] = df['Article Content'].apply(count_total_words)\n",
    "df['Complex Word Count'] = df['Article Content'].apply(count_complex_words)\n",
    "df['Syllable Per Word'] = df['Article Content'].apply(count_syllables_per_word)\n",
    "df['Avg Sentence Length'] = df['Article Content'].apply(calculate_avg_sentence_length)\n",
    "df['Percentage of Complex Words'] = df['Article Content'].apply(calculate_percentage_complex_words)\n",
    "df['Fog Index'] = df.apply(lambda row: calculate_fog_index(row['Avg Sentence Length'], row['Percentage of Complex Words']), axis=1)\n",
    "df['Avg Number of Words Per Sentence'] = df['Article Content'].apply(calculate_avg_words_per_sentence)\n",
    "df['Personal Pronouns'] = df['Article Content'].apply(count_personal_pronouns)\n",
    "df['Avg Word Length'] = df['Article Content'].apply(calculate_avg_word_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9814392c-3542-4eb7-9d84-f52d98943416",
   "metadata": {},
   "source": [
    "##### Let's Save the Extracted and Analysis data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dde9188c-ef9a-46a9-aa2a-804f75779543",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\'Web_Scraping_and_Text_Data_Analysis.xlsx\', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1ca226-b637-4a6a-b58c-49775e10557b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb68e2d-c26c-4f06-bd38-bb041c49b21f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d25a391-44eb-4a0b-9df4-ed83f92dd003",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995a4277-3242-4337-ac72-0abcc69be063",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9e39ce-dd6b-4d12-9adf-6b0347a3d8ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac431816-5a77-4d1c-a075-66cc451a8996",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dfe579-a3b4-4534-99fa-228592172f12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69de8271-47c5-4fba-9b66-177529916539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba2f2d7-2961-4a03-8d7e-0d3389c42a33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0f8424-7133-4ce0-a19a-f84afe6eb250",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8cefe4-352e-43a7-a819-e968e0111f67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e0134b-c029-469c-b504-4bd2efde9f15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c2fb4f-81cd-4663-8407-6e8f4b11c73c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6acbcba-3f7f-490d-9d33-7f0f8c73bb59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0accfdac-138e-4a03-8b21-cf3c691c7c74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
